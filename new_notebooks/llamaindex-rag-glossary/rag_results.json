{"question":{"0":"What is the similarity map?","1":"How can I install Renumics Spotlight?","2":"I want to find duplicates in my data. How can I do this?","3":"Which possibilities regarding cluster analysis does Spotlight offer and how can it help me?","4":"I want to identify label issues for my audio use case? Which features and visualizations can help me and how could the process look?","5":"What are the main UI elements that help me with my use case in Spotlight?","6":"What is the Inspector and how does it play together with other components?","7":"What is sliceguard and how does it play together with Spotlight?","8":"I want to look at my audio data. Which possibilities does Spotlight offer?","9":"How can I customize the data visualization in Renumics Spotlight?","10":"Which possibilities for data loading and export does Renumics Spotlight offer?"},"ground_truth":{"0":"The similarity map is a dimensionality reduction plot that displays high dimensional data like embeddings in 2D space. It can be very useful for getting a first overview on dataset structure and possible problems, before diving into more detailed analysis.","1":"Just run pip install renumics-spotlight. After that, start it by running spotlight from the CLI or use the Python interface via spotlight.show.","2":"You can either just do that exploratively, e.g. by placing data on the similarity map and checking if nearby datapoints are identical via the inspector. You can also detect duplicates with specialized tooling like annoy and display the results in spotlight via spotlight.show.","3":"Spotlight offers the similarity map, which can display embeddings or feature arrays in 2D. You can then identify clusters visually and browse through them while looking at the underlying data such as images or audio files in the inspector.","4":"Similarity Map, Inspector and Filtering can help you with that. You could simply place your data points on the similarity map using an general purpose audio embedding. You could then color the data points by label. If there are clusters with mixed labels, that's a sign for label issues. You could also train a model on the existing labels and then color by model error, which is a great way of pinpointing the issues even faster.","5":"The main elements are the table for exploring data points by their metadata, the inspector for displaying unstructured data, the filter bar for filtering the data, The similarity map as an overview plot on the dataset structure, and potentially multiple additional visualizations such as histograms or scatterplots, as well as metric display elements such as a confusion matrix.","6":"It is Spotlight's UI component that is able to display unstructured data such as images, audio files, text, and more. It can be used to easily explore unstructured, multimodal datasets.","7":"It is a library that can automatically detect data clusters or slices where a machine learning model does not perform well. It uses Spoglight to display its detection results. It is also built by Renumics.","8":"You can display audio data in the inspector. Spotlight offers the visualizations audio player and spetrogram. It can also display windows in which a certain event in the audio data occurs. To visualize audio dataset structure you could use audio embeddings that are displayed on the Similarity Map.","9":"You can configure the UI flexibly via the UI. This involves configuring a layout with different widgets and plots, as well as adding lenses to the inspector widget. You can also save and load configured layouts either via json files or via the python API.","10":"Spotlight can load pandas dataframes and huggingface datasets via spotlight.show. It can also load hdf5 files (legacy) and csv files. The preferred way is directly loading pandas dataframes via spotlight.show."},"answer":{"0":" The similarity map is a data visualization component that projects datapoints onto a 2D map based on the similarity of selected columns. The similarity can be measured by pre-computed descriptions like embeddings or by one or multiple scalar columns in the dataset. Users can select columns to 'placeBy' in the settings to arrange the data on the map using UMAP or PCA algorithms. The map can be moved, zoomed, and selected points can be added or removed using mouse controls. Coloring and sizing can also be used to represent more columns on the map. The similarity map can account for similarities of filtered out points by default but can be configured to compute similarities only on filtered points. The similarity map can be added to the Spotlight layout using the `similaritymap()` function in the API.","1":" To install Renumics Spotlight, you can use pip by running the command `pip install renumics-spotlight` in your terminal or command prompt. It's recommended to install Spotlight in a separate virtual environment for your Python project. After installation, you can use the Python API to load your dataset and explore it using various visualization tools provided by Spotlight. For more details, you can refer to the documentation provided in the context information.","2":" To find duplicates in your data, you can use the Annoy library to compute nearest neighbors and their distances. Here's how you can do it:\n\n1. First, you need to install the required packages using pip:\n```python\n!pip install renumics-spotlight datasets annoy\n```\n2. Next, you can define a function `nearest_neighbor_annoy` that takes your DataFrame `df`, the name of the embedding column `embedding_name`, the distance threshold `threshold`, and the tree size for Annoy index `tree_size` as arguments:\n```python\ndef nearest_neighbor_annoy(df, embedding_name='embedding', threshold=0.3, tree_size=100):\n    # ... function implementation here ...\n```\n3. In the function implementation, you can create an AnnoyIndex object with the length of your embeddings and the 'angular' distance type. Then, you can add each item to the index with its corresponding index and embedding. After that, you can build the AnnoyIndex tree with the specified tree size.\n4. Next, you can create an empty DataFrame `df_nn` to store the nearest neighbor information. Then, you can use the `get_nns_by_item` method of the AnnoyIndex object to find the indices of the nearest neighbors for each data point in your original DataFrame `df`. You can then use these indices to extract the corresponding nearest neighbor paths from the 'image' column of your DataFrame `df`.\n5. Finally, you can join the original DataFrame `df` with the nearest neighbor DataFrame `df_nn` using the concatenate method along the axis=1 to get the final DataFrame with the nearest neighbor information.\n6. To inspect and remove duplicates using Spotlight, you can load the final DataFrame into Spotlight using the `show` method and setting the appropriate data types for the 'image' and 'embedding' columns.\n\nHere's the complete code for the `nearest_neighbor_annoy` function:\n```python\nimport datasets\nfrom renumics import spotlight\nfrom annoy import AnnoyIndex\nimport pandas as pd\nimport requests\n\ndef nearest_neighbor_annoy(df, embedding_name='embedding', threshold=0.3, tree_size=100):\n    embs = df[embedding_name]\n\n    t = AnnoyIndex(len(embs[0]), 'angular')\n\n    for idx, x in enumerate(embs):\n        t.add_item(idx, x)\n\n    t.build(tree_size)\n\n    images = df['image']\n\n    df_nn = pd.DataFrame()\n\n    nn_id = [t.get_nns_by_item(i,2)[1] for i in range(len(embs))]\n    df_nn['nn_id'] = nn_id\n    df_nn['nn_image'] = [images[i] for i in nn_id]\n    df_nn['nn_distance'] = [t.get_distance(i, nn_id[i]) for i in range(len(embs))]\n    df_nn['nn_flag'] = (df_nn.nn_distance < threshold)\n\n    df = pd.concat([df, df_nn], axis=1)\n\n    return df\n```\n7. To use the function, you can load your dataset from Huggingface hub, convert it to a Pandas DataFrame, and then call the `nearest_neighbor_annoy` function with the appropriate arguments:\n```python\ndataset = datasets.load_dataset(\"renumics\/cifar100-enriched\", split=\"train\")\ndf = dataset.to_pandas()\ndf_nn = nearest_neighbor_annoy(df)\n```\n8. Finally, you can inspect the results using Spotlight:\n```python\ndf_show = df.drop(columns=['embedding', 'probabilities'])\nlayout_url = \"https:\/\/raw.githubusercontent.com\/Renumics\/spotlight\/playbook_initial_draft\/playbook\/rookie\/duplicates_annoy.json\"\nresponse = requests.get(layout_url)\nlayout = spotlight.layout.nodes.Layout(**json.loads(response.text))\nspotlight.show(df_show, dtype={\"image\": spotlight.Image, \"embedding_reduced\": spotlight.Embedding}, layout=layout)\n```","3":" Spotlight offers various possibilities for cluster analysis through its UI components. It includes the Inspector and Data Table to help analyze individual datapoints, as well as other visualization options like the Similarity Map, Scatter Plot, and Histogram to understand the overall distribution of your data. The Filter Bar allows filtering and selecting datapoints, which can affect how data is presented in other components and aid in analyzing and reasoning about your data. By leveraging model results such as predictions or embeddings, Spotlight can help uncover critical data samples and clusters, making it an effective tool for exploratory data analysis, model debugging, and monitoring.","4":" To identify label issues for your audio use case, you can utilize the `sliceguard` library and its companion tool `Spotlight`. This approach will help you quickly detect problem clusters that can cause issues when training your audio classification model. The process involves the following steps:\n\n1. First, ensure you have the required dependencies installed:\n   ```\n   pip install renumics-spotlight sliceguard[all] scikit-learn\n   ```\n2. Load your dataset into a DataFrame containing paths to audio files and their corresponding labels. The DataFrame format should look like:\n   ```\n   +---------------------+---------+\n   |        audio        | emotion |\n   +---------------------+---------+\n   | \/path\/to\/audio1.wav | joy     |\n   | \/path\/to\/audio2.wav | anger   |\n   | \/path\/to\/audio3.wav | joy     |\n   | ...                 |         |\n   +---------------------+---------+\n   ```\n3. Use `sliceguard` to detect issues:\n   ```python\n   from renumics import spotlight\n   from sliceguard import SliceGuard\n   from sliceguard.data import from_huggingface\n   from sklearn.metrics import accuracy_score\n\n   df = from_huggingface(\"renumics\/emodb\")\n   sg = SliceGuard()\n   issues = sg.find_issues(df, features=[\"audio\"], y=\"emotion\", metric=accuracy_score)\n   ```\n4. Generate a report of the detected issues:\n   ```python\n   report_df, spotlight_data_issues, spotlight_dtypes, spotlight_layout = sg.report(no_browser=True)\n   ```\n5. Visualize the detected issues using Spotlight:\n   ```python\n   spotlight.show(report_df, dtype=spotlight_dtypes, issues=spotlight_data_issues, layout=spotlight_layout)\n   ```\n\nThis process will help you identify label inconsistencies, outliers, and audio-specific issues that can be problematic for your audio classification model. The visualizations provided by Spotlight will give you insights into these issues, making it easier to address them.","5":" The main UI elements that can help you with your use case in Spotlight are the Inspector, Data Table, Filter Bar, Similarity Map, Scatter Plot, and Histogram. These components offer different perspectives on your data and can be added, removed, and rearranged to create customized inspection layouts. The Inspector and Data Table are particularly useful for analyzing individual datapoints, while the Similarity Map, Scatter Plot, and Histogram provide insights into the overall distribution of your data. The Filter Bar allows you to filter your data based on feature values, which can greatly aid in analyzing and reasoning about your data.","6":" The Inspector is a widget in Spotlight, a data exploration tool, that allows users to examine and edit the features of individual data points in their dataset. It provides multiple views for different modalities such as text, audio, video, and 3D geometry data, enabling users to explore data points in depth and make changes directly within the view. The Inspector is an essential tool for working with multimodal datasets and gives users greater control over the structure and quality of their data. It can be configured to represent each datapoint in various ways using components like Value view, Select view, Switch component, Image viewer, Audio Player, Spectrogram, Video Player, and Mesh Viewer. These components can be added to the Inspector using the inspector() function in the Spotlight library with preconfigured viewers (lenses).","7":" Sliceguard is a library used to identify data segments where machine learning models perform anomalously, also known as data slices. It helps in exploring these data slices to find model failure modes and problematic data segments. Sliceguard can be used with Spotlight, which is a data exploration tool for understanding data distributions and identifying issues. Together, they provide an interactive way to explore data slices and gain insights into the data that may be causing issues for machine learning models.","8":" Based on the provided context information, Spotlight offers two possibilities to view audio data: the `audio()` function and the `spectrogram()` function. The `audio()` function adds an audio viewer to the Spotlight inspector widget, while the `spectrogram()` function adds an audio spectrogram viewer to the Spotlight inspector widget. Both functions support a single column of type `spotlight.Audio` with an optional second column of type `spotlight.Window`.","9":" Renumics Spotlight provides various UI components to help you analyze and understand individual datapoints in your dataset. These components include the Inspector and Data Table. For customizing the data visualization in Renumics Spotlight, you can configure the Inspector view by choosing how each datapoint should be represented. The main configuration options for the Inspector view are available in the upper right corner of the view. There are visualization and interaction components available for many different data types such as Text data, Categorical data, Image data, Audio data, etc. You can refer to the provided documentation for each data type to learn about the available components and their usage. Additionally, you can save your preferred layout once you have identified a component layout that works well for you, for quick loading in the future without having to reconfigure the UI each time.","10":" Renumics Spotlight offers various possibilities for data loading and export. It supports loading data from Pandas dataframes, Huggingface datasets, and Spotlight datasets based on the HDF5-Format. The Python API and Command Line Interface (CLI) can be used to load data. When loading via CLI, you can specify a file to be loaded. With the Python API, you can also load in-memory datasets, which is useful when working in a notebook.\n\nHuggingface datasets have a rich semantic description of the feature columns, allowing Spotlight to parse data type descriptions and label mappings automatically. This simplifies creating a visualization, which can be done by importing the dataset using the Huggingface library and then using the `spotlight.show()` function.\n\nExploring raw unstructured datasets often yields little insight. Enriching datasets with model results such as predictions or embeddings can help uncover critical data samples and clusters. These enrichments can be stored in a separate dataframe and then joined with the raw dataset using the Python API. For example, you can load a Huggingface dataset and its corresponding results dataset using the `datasets` library and then concatenate them using the `concatenate_datasets()` function. Depending on the task at hand, you may want to set up a suitable visualization layout in the GUI or via the API."},"contexts":{"0":["---\ntags: []\nid: similarity-map\nsidebar_position: 30\nslug: \/docs\/custom-visualizations\/ui-components\/similarity-map\n---\n\nimport Video from \"@site\/src\/components\/DocsVideo\";\n\n# Similarity Map\n\nThe Similarity Map projects the datapoints onto a 2D Map based on the similarity of the selected columns to place by. The Similarity is measured by either pre computed descriptions (e.G. embeddings retrieved from a ML workflow) or based on one or multiple scalar columns in the dataset.\n\n## Placement\n\nSimply select one ore more columns to \u2018placeBy\u2019 in the settings and watch the dimension reduction arranging the data on the map.\n\nIn order to compute the dimension reduction we offer two different algorithms:\n[UMAP](https:\/\/arxiv.org\/abs\/1802.03426) and [PCA](https:\/\/en.wikipedia.org\/wiki\/Principal_component_analysis).\nIf the columns values have different distributions,\nThe reduction might run into problems and overestimate the impact of some columns.\nTo counteract this problem, a robust distance metric is available in the advanced settings.\n\nPer default only reduced settings are available to control UMAP\nwhich help you in configuring the reduction to place the samples with more weight on local vs. global similarities.\nHowever, there also are advanced settings available in order give more control over the dimension reduction.\n\nAt the moment the samples can be placed by number columns and Embedding\/Array columns.\n\n<Video\n    basePath=\"\/data\/docs\/ui-components\/similarity-map\/placement\"\n    extensions={[\"webm\", \"mp4\", \"ogg\"]}\n    widths={[256, 512, 720]}\n    annotation=\"_fsd50k_ - place datapoints on the **similarity map** based on a column\"\n\/>\n\n## Controls\n\nThe map can be moved and zoomed by using the mouse.\n\nZooming can be done with the mousewheel, resetting zoom can be done with the `Fit points` button.\n\nThe map can be moved by clicking the `middle mouse` button or with the `left mouse` button if the `alt` key is pressed simultaneously.\n\n<Video\n    basePath=\"\/data\/docs\/ui-components\/similarity-map\/controls\"\n    extensions={[\"webm\", \"mp4\", \"ogg\"]}\n    widths={[256, 512, 720]}\n    annotation=\"_fsd50k_ - navigate in the **similarity map**\"\n\/>\n\n## Selection\n\nSimilar to the [data table](data-table), a single row can be added or removed from the selection.\n\nIn order to select a single point simply click on it. This will reset any selection and exclusively select the clicked point.\n\nTo add a point to a selection click on it while pressing `shift` and to remove it press `ctrl` while clicking on it.\n\nThese steps also apply for multiple points. By pressing the left mouse button and moving the mouse,\na selection rectangle will be shown. On releasing the left mouse button, the selection will be applied in the same fashion as it is done for a single point.\n\n<Video\n    basePath=\"\/data\/docs\/ui-components\/similarity-map\/selection\"\n    extensions={[\"webm\", \"mp4\", \"ogg\"]}\n    widths={[256, 512, 720]}\n    annotation=\"_fsd50k_ - select and deselect points in the **similarity map**\"\n\/>\n\n## Coloring and Sizing\n\nColoring and sizing can be used to represent more columns on the map. The coloring palette can be altered in the global coloring settings.\n\n<Video\n    basePath=\"\/data\/docs\/ui-components\/similarity-map\/coloring-scaling\"\n    extensions={[\"webm\", \"mp4\", \"ogg\"]}\n    widths={[256, 512, 720]}\n    annotation=\"_fsd50k_ - colorize and scale datapoints on the **similarity map** based on a column\"\n\/>\n\n## Filtering\n\nPer default the similarity map also accounts for similarities of filtered out points.\nIn order to compute the similarities only on the filtered points,\n`hide unfiltered` has to be checked.\n\n<Video\n    basePath=\"\/data\/docs\/ui-components\/similarity-map\/filtering\"\n    extensions={[\"webm\", \"mp4\", \"ogg\"]}\n    widths={[256, 512, 720]}\n    annotation=\"_fsd50k_ - show\/hide filtered datapoints on the **similarity map**\"\n\/>","`similaritymap(name=None, columns=None, reduction_method=None, color_by_column=None, size_by_column=None, filter=False, *, umap_metric=None, umap_balance=None, pca_normalization=None)` {similaritymap()}\n\nAdd configured similarity map to Spotlight layout.","---\nsidebar_position: 40\nslug: \/docs\/data-centric-ai\/playbook\n---\n\nimport Tabs from \"@theme\/Tabs\";\nimport TabItem from \"@theme\/TabItem\";\nimport Video from \"@site\/src\/components\/ResponsiveVideo\";\nimport PlaybookChoooser from \"@site\/src\/components\/PlaybookChooser\";\n\n# \ud83c\udfc0 Playbook\n\nThis is a collection of useful data curation workflows (_plays_) on unstructured data.\nWe distinguish between basic data enrichment workflows (_rookie plays_),\nestablished data curation solutions (_veteran plays_) and current state-of-the-art techniques (_all-star plays_).\n\n<PlaybookChoooser \/>\n\n### Rookie plays\n\n-   [Create embeddings with Huggingface](.\/create_huggingface_embedding.mdx)\n-   [Create embeddings with towhee](.\/create_towhee_embedding.mdx)\n-   [Extract decision boundary based on probability ratios](.\/detect_decision_boundary.mdx)\n\n### Veteran plays\n\n-   [Detect duplicates with Annoy](.\/detect_duplicates_annoy.mdx)\n-   [Detect leakage with Annoy](.\/detect_leakage_annoy.mdx)\n-   [Detect data drift](.\/drift_kcore.mdx)\n-   [Detect label errors](.\/label_errors_cleanlab.mdx)\n-   [Detect outliers](.\/outlier_cleanlab.mdx)\n-   [Detect image error patterns](.\/cv_issues.mdx)\n\n### All-star plays\n\n-   [Find data slices](.\/find_data_slices_sliceguard.mdx)"],"1":["---\nslug: \/docs\/development\ntitle: Contributing to Spotlight\n---\n\n## Setting up Spotlight for development\n\nIn order to start development in spotlight as of now the easiest method is\nto checkout the repository and install development dependencies.\n\nThe spotlight backend is a [FastAPI](https:\/\/fastapi.tiangolo.com\/em\/) server written in `python`, serving the dataset\nand the frontend.<br \/>\nThe frontend is a [React](https:\/\/reactjs.org\/) application written in `typescript`.\n\nTherefore, for development, you'll need to install both `python` together with `poetry`\nand `nodejs` with `pnpm` to get started.\n\n:::info\nIf you want to contribute to spotlight in any form and struggle anywhere along the way please reach out to us.\n\nWe are more than happy to help out and guide you through the process.\n:::\n\n### Install dependencies\n\n#### Developing on Linux\n\nInstall [python3](https:\/\/www.python.org\/) together with [poetry](https:\/\/python-poetry.org\/docs\/)\n\n```bash\nsudo apt update\nsudo apt install python3 python3-dev\ncurl -sSL https:\/\/install.python-poetry.org | python3 -\n```\n\nInstall [nodejs](https:\/\/nodejs.org\/en\/) together with [pnpm](https:\/\/pnpm.io\/)\n\n```bash\nsudo apt install nodejs\ncurl -fsSL https:\/\/get.pnpm.io\/install.sh | sh -\n\n# check your installed node version\nnode -v\n```\n\nWe recommend using at least node version 18.\nIn order to install the latest version check out [NodeSource on Github](https:\/\/github.com\/nodesource\/distributions)\n\nYou might need to restart your terminal in order to use `pnpm` as a command.\n\n### Setup Spotlight Repository\n\nVisit the [spotlight repository](https:\/\/github.com\/Renumics\/spotlight) and click **Fork**.\nSetup your forked repository by cloning it and adding spotlight as an additional remote.\n\n```bash\ngit clone https:\/\/github.com\/YOUR_GIT_USERNAME\/spotlight.git\ncd spotlight\ngit remote add upstream https:\/\/github.com\/renumics\/spotlight.git\n```\n\nInside the spotlight repository you'll find a `Makefile` which contains all the commands to get you started.\nFirst install development dependencies and pre-commit.\n\n```bash\nmake init\n```\n\nand run the development server\n\n```bash\nmake dev\n```\n\n## Submit your work\n\nPlease make sure that you have [pre-commit](https:\/\/pre-commit.com\/) hooks installed and\nthat the hooks successfully ran for the added changes.<br \/>\nThis can be verified by creating a commit and checking if automated tests are run before the commit is created.\n\n```bash\npoetry run pre-commit install --hook-type pre-commit\npoetry run pre-commit install --hook-type pre-push\n```\n\nSubmit your improvements, fixes and new features to Spotlight by creating a\n[Pull Request](https:\/\/opensource.guide\/how-to-contribute\/#opening-a-pull-request).\n\n## Using [direnv](https:\/\/direnv.net\/)\n\nIn order to make development easier [direnv](https:\/\/direnv.net\/) can be used to automatically setup the environment\non entering the spotlight folder.\n\nThe provided [.envrc](https:\/\/github.com\/Renumics\/spotlight\/blob\/main\/.envrc) file automatically activates\nthe poetry environment and sets environment variables in .env and .env.local.","`inspector(name=None, lenses=None, num_columns=4)` {inspector()}\n\nAdd an inspector widget with optionally preconfigured viewers (lenses).\n\n Example\n\n```python\n>>> from renumics.spotlight import layout\n>>> from renumics.spotlight.layout import lenses\n>>> spotlight_layout = layout.layout(\n...     layout.inspector(\n...         \"My Inspector\",\n...         [\n...             lenses.scalar(\"bool\"),\n...             lenses.scalar(\"float\"),\n...             lenses.scalar(\"str\"),\n...             lenses.scalar(\"datetime\"),\n...             lenses.scalar(\"category\"),\n...             lenses.scalar(\"int\"),\n...             lenses.text(\"str\", name=\"text\"),\n...             lenses.html(\"str\", name=\"HTML (safe)\"),\n...             lenses.html(\"str\", name=\"HTML\", unsafe=True),\n...             lenses.markdown(\"str\", name=\"MD\"),\n...             lenses.array(\"embedding\"),\n...             lenses.array(\"window\"),\n...             lenses.array(\"array\"),\n...             lenses.sequences(\"sequence\"),\n...             lenses.sequences([\"sequence1\", \"sequence2\"], name=\"sequences\"),\n...             lenses.mesh(\"mesh\"),\n...             lenses.image(\"image\"),\n...             lenses.video(\"video\"),\n...             lenses.audio(\"audio\"),\n...             lenses.audio(\"audio\", window_column=\"window\", name=\"windowed audio\"),\n...             lenses.spectrogram(\"audio\"),\n...             lenses.spectrogram(\n...                 \"audio\",\n...                 window_column=\"window\",\n...                 name=\"windowed spectrogram\",\n...             ),\n...         ],\n...         num_columns=2,\n...     )\n... )\n```","---\nslug: \/docs\/getting-started\nsidebar_position: 10\n---\n\nimport ReactPlayer from \"react-player\";\nimport Tabs from \"@theme\/Tabs\";\nimport TabItem from \"@theme\/TabItem\";\n\n# \ud83d\ude80 Getting Started\n\n> Spotlight helps you to **understand unstructured datasets** fast. You can create **interactive visualizations** from your dataframe with just a few lines of code. You can also leverage data enrichments (e.g. embeddings, prediction, uncertainties) to **identify critical clusters** in your data.\n\n![Dataframe example](\/img\/speech_commands_vis_s.gif)\n\n## What you'll need\n\n-   [Python](https:\/\/www.python.org\/downloads\/) version 3.8-3.11\n\n## Install Spotlight via [pip](https:\/\/packaging.python.org\/en\/latest\/key_projects\/#pip)\n\n```bash\npip install renumics-spotlight\n```\n\n> We recommend installing Spotlight and everything you need to work on your data in a separate [virtual environment](https:\/\/docs.python.org\/3\/tutorial\/venv.html)\n\n## Load your first dataset\n\nData can be loaded into Spotlight either from memory (as Python object) or from file. We currently support Pandas dataframes as well as Huggingface datasets and our own HDF5-based dataset format. Depending on the format, unstructured data samples are either stored directly in the dataframe or as a path to a separate file.\n\nThis is an example how your dataframe might look like:\n\n![Dataframe example](\/img\/dataframe_head_sample.png)\n\nYou can directly load your dataframe either via the Python API or the command line interface (CLI):\n\n<Tabs groupId=\"interface\">\n<TabItem value=\"python\" label=\"python\" default>\n\n```python\nimport pandas as pd\nfrom renumics import spotlight\n\ndf = pd.read_csv(\"https:\/\/renumics.com\/data\/mnist\/mnist-tiny.csv\")\nspotlight.show(df, dtype={\"image\": spotlight.Image, \"embedding\": spotlight.Embedding})\n```\n\n-   `pd.read_csv` loads a sample csv file as a pandas [DataFrame](https:\/\/pandas.pydata.org\/docs\/reference\/api\/pandas.DataFrame.html).\n-   `spotlight.show` opens up spotlight in the browser with the pandas dataframe ready for you to explore.\n-   The `dtype` argument specifies custom column types for the browser viewer.\n\n<\/TabItem>\n<TabItem value=\"bash\" label=\"CLI\">\n\n```bash\ncurl https:\/\/renumics.com\/data\/mnist\/mnist-tiny.csv -o mnist-tiny.csv\nspotlight mnist-tiny.csv --dtype image=Image --dtype embedding=Embedding\n```\n\n<\/TabItem>\n<\/Tabs>\n\n## Load a [Hugging Face](https:\/\/huggingface.co\/) dataset\n\nHuggingface datasets have a rich semantic description of the feature columns. Spotlight can thus parse data type descriptions and label mappings automatically. This means that creating a visualization is typically as simple as:\n\n```python\nimport datasets\nfrom renumics import spotlight\n\nds = datasets.load_dataset('speech_commands', 'v0.01', split='all')\n\nspotlight.show(ds)\n```\n\n## Exploring an enriched dataset with custom visualization layouts\n\nExploring raw unstructured datasets often yield little insights. Leveraging model results such as predictions or embeddings can help to uncover critical data samples and clusters. In practice, these enrichments can be stored in a separate dataframe and then joined with the raw dataset. Here is an example from the Huggingface hub:\n\n```python\nimport datasets\n\nds = datasets.load_dataset('speech_commands', 'v0.01', split='all')\nds_results = datasets.load_dataset('renumics\/speech_commands-ast-finetuned-results', 'v0.01', split='all')\nds = datasets.concatenate_datasets([ds, ds_results], axis=1)\n```\n\nDepending on the task at hand (e.g. EDA, model debugging, monitoring), you probably want to set up a suitable visualization layout. You can do so in the GUI and via API."],"2":["---\ntags: []\nid: duplicates-annoy\nsidebar_position: 35\nslug: \/docs\/data-centric-ai\/playbook\/duplicates-annoy\n---\n\nimport Tabs from \"@theme\/Tabs\";\nimport TabItem from \"@theme\/TabItem\";\nimport Video from \"@site\/src\/components\/DocsVideo\";\n\n# Detect duplicates with Annoy\n\nWe use embeddings to detect duplicates by computing nearest neighbors with the Annoy library. Although the example is based on image embeddings, the basic play is independent of the data type.\n\n> Use Chrome to run Spotlight in Colab. Due to Colab restrictions (e.g. no websocket support), the performance is limited. Run the notebook locally for the full Spotlight experience.\n\n<a\n    target=\"_blank\"\n    href=\"https:\/\/colab.research.google.com\/github\/Renumics\/spotlight\/blob\/main\/playbook\/veteran\/duplicates_annoy.ipynb\"\n>\n    <img\n        src=\"https:\/\/colab.research.google.com\/assets\/colab-badge.svg\"\n        alt=\"Open In Colab\"\n    \/>\n<\/a>\n\n<Tabs groupId=\"usedDatasets\" >\n<TabItem value=\"inputs\" label=\"inputs\">\n\n-   `df['embedding']` contain the [embeddings](..\/glossary\/index.mdx#embedding) for each data sample\n\n<\/TabItem>\n<TabItem value=\"outputs\" label=\"outputs\">\n\n-   `df['nn_id']` contains the sample id for the [nearest neighbor](..\/glossary\/index.mdx#nearest-neighbor) in the embedding space.\n-   `df['nn_image']` contains the path to the [image](..\/glossary\/index.mdx#image-data) that is the nearest neighbor.\n-   `df['nn_distance']` contains distance to the nearest neighbor.\n-   `df['nn_flag']` contains a flag that indicates if the sample is a near-duplicate according to the given threshold.\n\n<\/TabItem>\n<TabItem value=\"parameters\" label=\"parameters\">\n* `threshold` denotes the distance threshold when a data sample is considered a near-duplicate.\n* `tree_size` is an internal parameter for the Annoy library that calibrates the speed-efficiency tradeoff. More trees gives higher precision when querying.\n<\/TabItem>\n<\/Tabs>\n\n![Spotlight_screenshot_decision_boundary](\/playbook\/duplicates_annoy_screenshot.png)\n\n## Imports and play as copy-n-paste functions\n\n<details>\n    <summary># Install dependencies<\/summary>\n\n```python\n#@title Install required packages with PIP\n\n!pip install renumics-spotlight datasets annoy\n```\n\n<\/details>\n\n<details>\n    <summary># Play as copy-n-paste functions<\/summary>\n\n```python\n#@title Play as copy-n-paste functions\n\nimport datasets\nfrom renumics import spotlight\nfrom annoy import AnnoyIndex\nimport pandas as pd\nimport requests\n\n\ndef nearest_neighbor_annoy(df, embedding_name='embedding', threshold=0.3, tree_size=100):\n\n    embs = df[embedding_name]\n\n    t = AnnoyIndex(len(embs[0]), 'angular')\n\n    for idx, x in enumerate(embs):\n          t.add_item(idx, x)\n\n    t.build(tree_size)\n\n    images = df['image']\n\n    df_nn = pd.DataFrame()\n\n    nn_id = [t.get_nns_by_item(i,2)[1] for i in range(len(embs))]\n    df_nn['nn_id'] = nn_id\n    df_nn['nn_image'] = [images[i] for i in nn_id]\n    df_nn['nn_distance'] = [t.get_distance(i, nn_id[i]) for i in range(len(embs))]\n    df_nn['nn_flag'] = (df_nn.nn_distance < threshold)\n\n\n    return df_nn\n```\n\n<\/details>\n\n## Step-by-step example on CIFAR-100\n\n### Load CIFAR-100 from Huggingface hub and convert it to Pandas dataframe\n\n```python\ndataset = datasets.load_dataset(\"renumics\/cifar100-enriched\", split=\"train\")\ndf = dataset.to_pandas()\n```\n\n### Compute nearest neighbors including distances\n\n```python\ndf_nn = nearest_neighbor_annoy(df)\ndf = pd.concat([df, df_nn], axis=1)\n```\n\n### Inspect and remove duplicates with Spotlight\n\n```python\ndf_show = df.drop(columns=['embedding', 'probabilities'])\nlayout_url = \"https:\/\/raw.githubusercontent.com\/Renumics\/spotlight\/playbook_initial_draft\/playbook\/rookie\/duplicates_annoy.json\"\nresponse = requests.get(layout_url)\nlayout = spotlight.layout.nodes.Layout(**json.loads(response.text))\nspotlight.show(df_show, dtype={\"image\": spotlight.Image, \"embedding_reduced\": spotlight.Embedding}, layout=layout)\n```","`check_column_name(name)` {Dataset.check_column_name()}\n\nCheck a column name.\n\nInstance variables","Leveraging model results such as predictions or embeddings can help to uncover critical data samples and clusters. In practice, these enrichments can be stored in a separate dataframe and then joined with the raw dataset. Here is an example from the Huggingface hub:\n\n```python\nimport datasets\n\nds = datasets.load_dataset('speech_commands', 'v0.01', split='all')\nds_results = datasets.load_dataset('renumics\/speech_commands-ast-finetuned-results', 'v0.01', split='all')\nds = datasets.concatenate_datasets([ds, ds_results], axis=1)\n```\n\nDepending on the task at hand (e.g. EDA, model debugging, monitoring), you probably want to set up a suitable visualization layout. You can do so in the GUI and via API. We also ship starter layouts for common tasks that you can use out of the box:\n\n```python\nfrom renumics import spotlight\n\nlayout = spotlight.layouts.debug_classification(embedding='embedding', inspect={'audio': spotlight.dtypes.audio_dtype})\nspotlight.show(ds, dtype={'embedding': spotlight.Embedding}, layout=layout )\n```\n\n## Disclaimer\n\n<details>\n    <summary>Usage Tracking<\/summary>\n\nWe have added crash report and perfomance collection.<br \/>\n**We do NOT** collect user data other than an **anonymized Machine Id** obtained by py-machineid, and only log our own actions.<br \/>\n**We do NOT** collect folder names, dataset names, or row data of any kind only aggregate performance statistics like total time of a table_load, crash data, etc.<br \/>\nCollecting spotlight crashes will help us improve stability.<br \/>\n\n<br \/>\n<br \/>\nToo opt out of the crash report collection define an environment variable called\nSPOTLIGHT_OPT_OUT and set it to true.\n\ne.G.\n\n```bash\nexport SPOTLIGHT_OPT_OUT=true\n```\n\n<\/details>"],"3":["---\nsidebar_position: 40\nslug: \/docs\/configure-visualizations\/issues\n---\n\n# \ud83d\udd16 Defining data issues\n\nSpotlight allows to define highlight specific data clusters. These clusters can be defined by their respective row and\/or column ids an allow to quickly inspect critical issues in your data.","---\nsidebar_position: 30\nslug: \/docs\/configure-visualizations\n---\n\n# Configuring visualizations\n\nData inspection is critical for all steps in the ML development workflow. But it is also time consuming: First of all it takes effort to build and maintain custom visualization pipelines. Second of all, it takes time to sift through data manually.\n\nSpotlight enables you to build scalable data inspection workflows. With Spotlight you can\n\n-   Create visualizations for unstructured data with a single line of code\n-   Leverage model results to find relevant data segments quickly\n-   Create custom interaction templates for your team\n\nIn order to achieve this, the Spotlight UI provides a range of components that offer different perspectives on your data. You can add, remove, and rearrange these components as you wish to create a customized inspection layouts.\n\nOnce you have identified a component layout that works well for you, you can save it as a layout for future use. This way, you can quickly load your preferred layout whenever you need it, without having to reconfigure the UI each time.\n\n## UI components\n\nSpotlight includes the [Inspector](ui_components\/inspector_view.mdx) and the [Data Table](ui_components\/table_view.mdx) to help you in analyzing and understanding individual datapoints. Other visualization options like the [Similarity Map](ui_components\/similarity_map.mdx), the [Scatter Plot](ui_components\/scatter_plot.mdx) and the [Histogram](ui_components\/histogram.mdx) aide you to understand the overall distribution of your data.\n\nThe [Filter Bar](ui_components\/filter_bar.mdx) is a special component that allows you to filter your data based on the values of your features.<br\/>\nFiltering and selecting datapoints can affect how data is presented in the other components and therefore can greatly help you in analyzing and reasoning about your data.\n\nAn overview over all available components can be found in the [UI components](ui_components\/index.mdx) section.\n\n## Layout API\n\n## Issues widget","Leveraging model results such as predictions or embeddings can help to uncover critical data samples and clusters. In practice, these enrichments can be stored in a separate dataframe and then joined with the raw dataset. Here is an example from the Huggingface hub:\n\n```python\nimport datasets\n\nds = datasets.load_dataset('speech_commands', 'v0.01', split='all')\nds_results = datasets.load_dataset('renumics\/speech_commands-ast-finetuned-results', 'v0.01', split='all')\nds = datasets.concatenate_datasets([ds, ds_results], axis=1)\n```\n\nDepending on the task at hand (e.g. EDA, model debugging, monitoring), you probably want to set up a suitable visualization layout. You can do so in the GUI and via API. We also ship starter layouts for common tasks that you can use out of the box:\n\n```python\nfrom renumics import spotlight\n\nlayout = spotlight.layouts.debug_classification(embedding='embedding', inspect={'audio': spotlight.dtypes.audio_dtype})\nspotlight.show(ds, dtype={'embedding': spotlight.Embedding}, layout=layout )\n```\n\n## Disclaimer\n\n<details>\n    <summary>Usage Tracking<\/summary>\n\nWe have added crash report and perfomance collection.<br \/>\n**We do NOT** collect user data other than an **anonymized Machine Id** obtained by py-machineid, and only log our own actions.<br \/>\n**We do NOT** collect folder names, dataset names, or row data of any kind only aggregate performance statistics like total time of a table_load, crash data, etc.<br \/>\nCollecting spotlight crashes will help us improve stability.<br \/>\n\n<br \/>\n<br \/>\nToo opt out of the crash report collection define an environment variable called\nSPOTLIGHT_OPT_OUT and set it to true.\n\ne.G.\n\n```bash\nexport SPOTLIGHT_OPT_OUT=true\n```\n\n<\/details>"],"4":["---\ntags: []\nid: audio-classification\nslug: \/docs\/use-cases\/audio-classification\n---\n\n# Detect Problems in Audio Classification Datasets\n\nUse the [**sliceguard**](https:\/\/github.com\/Renumics\/sliceguard) library and [**Spotlight**](https:\/\/github.com\/Renumics\/spotlight) to quickly **detect problem clusters** that can cause issues when training your audio classification model. Shows problems such as:\n\n1. Label Inconsistencies\n2. Outliers and Errors\n3. Audio-specific Issues (e.g. clipping)\n\nGenerally it will show you everything that is hard to learn for a classification model. All you need is a dataframe containing **paths to audio files** and **labels**.\n\nFirst install the dependencies:\n\n```\npip install renumics-spotlight sliceguard[all] scikit-learn\n```\n\nThen run the following code to detect problematic clusters:\n\n```python\n# The Imports\nfrom renumics import spotlight\nfrom sliceguard import SliceGuard\nfrom sliceguard.data import from_huggingface\nfrom sklearn.metrics import accuracy_score\n\n# Load an Example Dataset as DataFrame\ndf = from_huggingface(\"renumics\/emodb\")\n\n# DataFrame Format:\n# +---------------------+---------+\n# |        audio        | emotion |\n# +---------------------+---------+\n# | \/path\/to\/audio1.wav | joy     |\n# | \/path\/to\/audio2.wav | anger   |\n# | \/path\/to\/audio3.wav | joy     |\n# | ...                 |         |\n# +---------------------+---------+\n\n# Detect Issues Using sliceguard\nsg = SliceGuard()\nissues = sg.find_issues(df, features=[\"audio\"], y=\"emotion\", metric=accuracy_score)\nreport_df, spotlight_data_issues, spotlight_dtypes, spotlight_layout = sg.report(\n    no_browser=True\n)\n\n# Visualize Detected Issues in Spotlight:\nspotlight.show(\n    report_df,\n    dtype=spotlight_dtypes,\n    issues=spotlight_data_issues,\n    layout=spotlight_layout,\n)\n\n```","{\n    \"label\": \"\ud83c\udf99\ufe0f Audio\",\n    \"position\": 10,\n    \"link\": {\n        \"type\": \"generated-index\",\n        \"slug\": \"docs\/use-cases\/audio\"\n    }\n}","---\ntags: []\nid: text-classification\nslug: \/docs\/use-cases\/text-classification\n---\n\n# Detect Problems in Text Classification Datasets\n\nUse the [**sliceguard**](https:\/\/github.com\/Renumics\/sliceguard) library and [**Spotlight**](https:\/\/github.com\/Renumics\/spotlight) to quickly **detect problem clusters** that can cause issues when training your text classification model. Shows problems such as:\n\n1. Label Inconsistencies\n2. Outliers and Errors\n3. Text-specific Issues (e.g. typos, empty samples)\n\nGenerally it will show you everything that is hard to learn for a classification model. All you need is a dataframe containing **texts** and **labels**.\n\nFirst install the dependencies:\n\n```\npip install renumics-spotlight sliceguard[all] scikit-learn\n```\n\nThen run the following code to detect problematic clusters:\n\n```python\n# The Imports\nfrom renumics import spotlight\nfrom sliceguard import SliceGuard\nfrom sliceguard.data import from_huggingface\nfrom sklearn.metrics import accuracy_score\n\n# Load an Example Dataset as DataFrame\ndf = from_huggingface(\"dair-ai\/emotion\")\n\n# DataFrame Format:\n# +-------+-------+\n# | text  | label |\n# +-------+-------+\n# | text1 | joy   |\n# | text2 | anger |\n# | text3 | joy   |\n# | ...   |       |\n# +-------+-------+\n\n# Detect Issues Using sliceguard\nsg = SliceGuard()\nissues = sg.find_issues(df, features=[\"text\"], y=\"label\", metric=accuracy_score)\nreport_df, spotlight_data_issues, spotlight_dtypes, spotlight_layout = sg.report(\n    no_browser=True\n)\n\n# Visualize Detected Issues in Spotlight:\nspotlight.show(\n    report_df,\n    dtype=spotlight_dtypes,\n    issues=spotlight_data_issues,\n    layout=spotlight_layout,\n)\n```"],"5":["---\nsidebar_position: 30\nslug: \/docs\/configure-visualizations\n---\n\n# Configuring visualizations\n\nData inspection is critical for all steps in the ML development workflow. But it is also time consuming: First of all it takes effort to build and maintain custom visualization pipelines. Second of all, it takes time to sift through data manually.\n\nSpotlight enables you to build scalable data inspection workflows. With Spotlight you can\n\n-   Create visualizations for unstructured data with a single line of code\n-   Leverage model results to find relevant data segments quickly\n-   Create custom interaction templates for your team\n\nIn order to achieve this, the Spotlight UI provides a range of components that offer different perspectives on your data. You can add, remove, and rearrange these components as you wish to create a customized inspection layouts.\n\nOnce you have identified a component layout that works well for you, you can save it as a layout for future use. This way, you can quickly load your preferred layout whenever you need it, without having to reconfigure the UI each time.\n\n## UI components\n\nSpotlight includes the [Inspector](ui_components\/inspector_view.mdx) and the [Data Table](ui_components\/table_view.mdx) to help you in analyzing and understanding individual datapoints. Other visualization options like the [Similarity Map](ui_components\/similarity_map.mdx), the [Scatter Plot](ui_components\/scatter_plot.mdx) and the [Histogram](ui_components\/histogram.mdx) aide you to understand the overall distribution of your data.\n\nThe [Filter Bar](ui_components\/filter_bar.mdx) is a special component that allows you to filter your data based on the values of your features.<br\/>\nFiltering and selecting datapoints can affect how data is presented in the other components and therefore can greatly help you in analyzing and reasoning about your data.\n\nAn overview over all available components can be found in the [UI components](ui_components\/index.mdx) section.\n\n## Layout API\n\n## Issues widget","---\nsidebar_position: 10\nslug: \/docs\/configure-visualizations\/ui-components\n---\n\nimport Tabs from \"@theme\/Tabs\";\nimport TabItem from \"@theme\/TabItem\";\nimport Video from \"@site\/src\/components\/ResponsiveVideo\";\n\n# \ud83c\udf9b UI components\n\n> If you want to follow these examples hands-on, you can use the Spotlight example instances fired up in the [\ud83d\ude80 Getting Started](\/docs\/getting-started) section.<br \/>\n> Examples will be annotated with the corresponding datasets name. You can use the tabs below for a quick reminder on how to load those datasets into Spotlight.\n\n<Tabs groupId=\"usedDatasets\" >\n<TabItem value=\"mnist\" label=\"mnist\">\n\n```bash\ncurl https:\/\/renumics.com\/docs\/data\/mnist\/mnist-tiny.csv -o mnist-tiny.csv\nspotlight mnist-tiny.csv --dtype image=Image --dtype embedding=Embedding --dtype label=Category\n```\n\n<\/TabItem>\n<TabItem value=\"fsd50k\" label=\"fsd50k\">\n\n```bash\ncurl https:\/\/spotlight.renumics.com\/data\/fsd50k\/fsd50k-tiny.csv -o fsd50k-tiny.csv\nspotlight fsd50k-tiny.csv --dtype window=Window --dtype embedding=Embedding --dtype audio=Audio\n```\n\n<\/TabItem>\n<\/Tabs>\n\n<br \/>\n\nSpotlight includes the [Inspector](.\/inspector_view.mdx) and the [Data Table](.\/table_view.mdx) to help you in analyzing and understanding individual datapoints. Other visualization options like the [Similarity Map](.\/similarity_map.mdx), the [Scatter Plot](.\/scatter_plot.mdx) and the [Histogram](.\/histogram.mdx) aide you to understand the overall distribution of your data.\n\nThe [Filter Bar](.\/filter_bar.mdx) is a special component that allows you to filter your data based on the values of your features.<br\/>\nFiltering and selecting datapoints can affect how data is presented in the other components and therefore can greatly help you in analyzing and reasoning about your data.\n\n### Data Table\n\nThe [data table](.\/table_view.mdx) often is the primary view on the data.\nIn addition to several options that control which data is displayed, the table view also allows to edit datapoints.\nThis includes the creation of new columns.\n\n### Inspector\n\nThe [inspector](.\/inspector_view.mdx) lets you examine individual data points in depth by providing multiple different views for many of the data types supported by Spotlight.\nIncluding images, audio, 3D meshes and more.\n\n### Similarity map\n\nThe [similarity map](.\/similarity_map.mdx) is a core element of most data-centric AI workflows.\nIt allows for to map a given vector to a scatter plot by using a dimensionality reduction via UMAP or PCA.\nDifferent normalization options are available to handle metadata and embeddings. Additionally, dots on the scatter plot can be colored and sized.\n\n### Scatter plot\n\nThe [scatter plot](.\/scatter_plot.mdx) is typically useful as a supporting view to determine correlation between metadata information.\nSeveral aspects of the view can be customized dot size and color.\n\n### Histogram\n\nThe [histogram view](.\/histogram.mdx) can be stacked to provide inside into data segments over two different dimensions.","---\nsidebar_position: 30\nslug: \/docs\/configure-visualizations\/layout-api\n---\n\n# \ud83d\uddc3\ufe0f Creating data inspection layouts\n\nSpotlight layouts can be created and loaded both through the UI and the Python API\n\n## Saving and loading layouts through the UI\n\n## Creating layouts through the Python API\n\nMore info can be found in the API documentation for the [`layout`](..\/..\/api\/spotlight\/layout\/) class."],"6":["---\ntags: []\nid: inspector\nsidebar_position: 20\nslug: \/docs\/custom-visualizations\/ui-components\/inspector\n---\n\nimport Video from \"@site\/src\/components\/DocsVideo\";\n\n# Inspector\n\nThe Inspector Widget is a component in Spotlight that lets you examine and edit the features of **individual data points** in your dataset.\nWith its multiple views for **different modalities**, the Inspector Widget provides a detailed look at a data point's underlying structure.\n\nThe Inspector Widget's views for text, audio, video, and 3D geometry data allow you to explore a data point's features in depth.\nYou can also edit those features directly within the view, making it easy to make changes to a data point without navigating to a separate screen.\nThe Inspector Widget is an essential tool for working with multimodal datasets and gives you greater control over the structure and quality of your data.\n\n:::info\n\nThe open source version of Renumics Spotlight allows you to inspect your data to find critical segments. The Pro version of Spotlight also enables data tagging and annotation.\n\n:::\n\nBy configuring the inspector view you can choose how each datapoint should be represented. The main configuration options are available in the upper right corner of the view:\n\n<Video\n    basePath=\"\/data\/docs\/ui-components\/inspector-view\/inspector-view-overview\"\n    extensions={[\"webm\", \"mp4\", \"ogg\"]}\n    widths={[256, 512, 720, 1280]}\n    annotation=\"_fsd50k_ - add and layout components in the **Inspector View**\"\n\/>\n\nThere are visualization and interaction components available for many different data types. These include:\n\n### Text data\n\n**Text data** can be represented either with a **Value** view or a **Select** view that features autocomplete.\n\n<!-- poster=\"\/data\/docs\/ui-components\/inspector-view\/text-value-field\/text-value-field.jpg\" -->\n\n<Video\n    basePath=\"\/data\/docs\/ui-components\/inspector-view\/text-value-field\"\n    extensions={[\"webm\", \"mp4\", \"ogg\"]}\n    widths={[256, 512]}\n    isPro={false}\n    poster=\"\/data\/docs\/ui-components\/inspector-view\/text-value-field-512.jpg\"\n    annotation=\"_fsd50k_ - display annotation in a **Value** view\"\n\/>\n\n<Video\n    className=\"pt-10\"\n    basePath=\"\/data\/docs\/ui-components\/inspector-view\/text-select-field\"\n    extensions={[\"webm\", \"mp4\", \"ogg\"]}\n    widths={[256, 512]}\n    isPro={true}\n    poster=\"\/data\/docs\/ui-components\/inspector-view\/text-select-field.jpg\"\n    annotation=\"_fsd50k_ - display and update annotation in a **Select** view\"\n\/>\n\n### Categorical data\n\n**Categorical data** is represented similarly to **Text data** with the exception, that when there are not too many categories, they can also be edited with a switch component.\n\n<Video\n    className=\"pt-10\"\n    basePath=\"\/data\/docs\/ui-components\/inspector-view\/category-switch-field\"\n    extensions={[\"webm\", \"mp4\", \"ogg\"]}\n    widths={[256, 512]}\n    isPro={true}\n    poster=\"\/data\/docs\/ui-components\/inspector-view\/category-switch-field-512.jpg\"\n    annotation=\"_mnist_ - display and update categories in a **Switch** view\"\n\/>\n\n### Image data\n\n**Image data** can be represented with an imager viewer that supports zooming and panning.\n\n<Video\n    className=\"pt-10\"\n    basePath=\"\/data\/docs\/ui-components\/inspector-view\/image-view\"\n    extensions={[\"webm\", \"mp4\", \"ogg\"]}\n    widths={[256, 512]}\n    isPro={false}\n    poster=\"\/data\/docs\/ui-components\/inspector-view\/image-view-512.jpg\"\n    annotation=\"_mnist_ - display and zoom\/pan images in a **Image** view\"\n\/>\n\n### Audio data\n\n**Audio data** can be represented with an _Audio Player_ or a _Spectrogram_. The _Audio Player_ supports annotating event windows in the data.","`inspector(name=None, lenses=None, num_columns=4)` {inspector()}\n\nAdd an inspector widget with optionally preconfigured viewers (lenses).\n\n Example\n\n```python\n>>> from renumics.spotlight import layout\n>>> from renumics.spotlight.layout import lenses\n>>> spotlight_layout = layout.layout(\n...     layout.inspector(\n...         \"My Inspector\",\n...         [\n...             lenses.scalar(\"bool\"),\n...             lenses.scalar(\"float\"),\n...             lenses.scalar(\"str\"),\n...             lenses.scalar(\"datetime\"),\n...             lenses.scalar(\"category\"),\n...             lenses.scalar(\"int\"),\n...             lenses.text(\"str\", name=\"text\"),\n...             lenses.html(\"str\", name=\"HTML (safe)\"),\n...             lenses.html(\"str\", name=\"HTML\", unsafe=True),\n...             lenses.markdown(\"str\", name=\"MD\"),\n...             lenses.array(\"embedding\"),\n...             lenses.array(\"window\"),\n...             lenses.array(\"array\"),\n...             lenses.sequences(\"sequence\"),\n...             lenses.sequences([\"sequence1\", \"sequence2\"], name=\"sequences\"),\n...             lenses.mesh(\"mesh\"),\n...             lenses.image(\"image\"),\n...             lenses.video(\"video\"),\n...             lenses.audio(\"audio\"),\n...             lenses.audio(\"audio\", window_column=\"window\", name=\"windowed audio\"),\n...             lenses.spectrogram(\"audio\"),\n...             lenses.spectrogram(\n...                 \"audio\",\n...                 window_column=\"window\",\n...                 name=\"windowed spectrogram\",\n...             ),\n...         ],\n...         num_columns=2,\n...     )\n... )\n```","<Video\n    className=\"pt-10\"\n    basePath=\"\/data\/docs\/ui-components\/inspector-view\/image-view\"\n    extensions={[\"webm\", \"mp4\", \"ogg\"]}\n    widths={[256, 512]}\n    isPro={false}\n    poster=\"\/data\/docs\/ui-components\/inspector-view\/image-view-512.jpg\"\n    annotation=\"_mnist_ - display and zoom\/pan images in a **Image** view\"\n\/>\n\n### Audio data\n\n**Audio data** can be represented with an _Audio Player_ or a _Spectrogram_. The _Audio Player_ supports annotating event windows in the data.\n\n<Video\n    className=\"pt-10\"\n    basePath=\"\/data\/docs\/ui-components\/inspector-view\/audio-window-view\"\n    extensions={[\"webm\", \"mp4\", \"ogg\"]}\n    widths={[256, 512]}\n    isPro={false}\n    poster=\"\/data\/docs\/ui-components\/inspector-view\/audio-window-view-512.jpg\"\n    annotation=\"_fsd50k_ - display and control audio with\/without an additional window\"\n\/>\n\n<Video\n    className=\"pt-10\"\n    basePath=\"\/data\/docs\/ui-components\/inspector-view\/audio-window-edit-view\"\n    extensions={[\"webm\", \"mp4\", \"ogg\"]}\n    widths={[256, 512]}\n    isPro={true}\n    poster=\"\/data\/docs\/ui-components\/inspector-view\/audio-window-edit-view-512.jpg\"\n    annotation=\"_fsd50k_ - edit audio window\"\n\/>\n\n### Video data\n\n**Video data** can be represented with an _Video Player_.\n\n### 3D Meshes\n\n**3D meshes** can be represented by a _Mesh Viewer_. This components supports surface coloring to display mesh properties (e.g. stresses) and animated meshes.\n\n### Boolean data\n\n**Boolean data** can be represented by a _Switch_ element to facilitate efficient data annotation and tagging.\n\n<Video\n    className=\"pt-10\"\n    basePath=\"\/data\/docs\/ui-components\/inspector-view\/bool-switch-view\"\n    extensions={[\"webm\", \"mp4\", \"ogg\"]}\n    widths={[256, 512]}\n    isPro={true}\n    poster=\"\/data\/docs\/ui-components\/inspector-view\/bool-switch-view-512.jpg\"\n    annotation=\"_fsd50k_ - edit bool values in **Switch** view\"\n\/>"],"7":["---\ntags: []\nid: sliceguard\nsidebar_position: 6055\nslug: \/docs\/data-centric-ai\/playbook\/data-slices-sliceguard\n---\n\nimport Tabs from \"@theme\/Tabs\";\nimport TabItem from \"@theme\/TabItem\";\nimport Video from \"@site\/src\/components\/DocsVideo\";\n\n# Find data slices with Sliceguard\n\nWe use [Sliceguard](https:\/\/github.com\/Renumics\/sliceguard) to identify data segments where our machine learning model performs anomalously (data slices). We interactively explore these data slices to find model failure modes and problematic data segments.\n\n> Use Chrome to run Spotlight in Colab. Due to Colab restrictions (e.g. no websocket support), the performance is limited. Run the notebook locally for the full Spotlight experience.\n\n<a\n    target=\"_blank\"\n    href=\"https:\/\/colab.research.google.com\/github\/Renumics\/spotlight\/blob\/main\/playbook\/allstar\/data-slices-sliceguard.ipynb\"\n>\n    <img\n        src=\"https:\/\/colab.research.google.com\/assets\/colab-badge.svg\"\n        alt=\"Open In Colab\"\n    \/>\n<\/a>\n\n<Tabs groupId=\"usedDatasets\" >\n<TabItem value=\"inputs\" label=\"inputs\">\n\n-   `categories` contains contain the names of the [features](..\/glossary\/index.mdx#features) and [metadata](..\/glossary\/index.mdx#metadata) columns that are to be analyzed\n-   `df['label']` contains the [label](..\/glossary\/index.mdx#label) for each data sample\n-   `df['prediction']` contains the [prediction](..\/glossary\/index.mdx#label) for each data sample\n-   `df['embedding']` contains the [embeddings](..\/glossary\/index.mdx#embedding) for each data sample (optional)\n\n<\/TabItem>\n\n<TabItem value=\"outputs\" label=\"outputs\">\n\n-   `df_slices contains a dataframe with a description of the issues found\n\n<\/TabItem>\n\n<TabItem value=\"parameters\" label=\"parameters\">\n* `category_types` (optional) describes the type of the features and metadata (\"raw\", \"nominal\", \"ordinal\", \"numerical\", \"embedding\").\n* `spotlight_dtype` (optional) describes data types for the visualization with Spotlight.\n<\/TabItem>\n<\/Tabs>\n\n![Spotlight_screenshot_drift_kcore](\/playbook\/screenshot_sliceguard.png)\n\n## Imports and play as copy-n-paste functions\n\n<details>\n    <summary># Install dependencies<\/summary>\n\n```python\n#@title Install required packages with PIP\n\n!pip install renumics-spotlight sliceguard datasets cleanvision\n```\n\n<\/details>\n\n<details>\n    <summary>Play as copy-n-paste functions<\/summary>\n\n```python\n#@title Play as copy-n-paste snippet\n\nfrom sklearn.metrics import accuracy_score\nimport pandas as pd\nimport datasets\nfrom renumics.spotlight import Image\nfrom sliceguard import SliceGuard\nfrom cleanvision.imagelab import Imagelab","def find_data_slices(df, categories, category_types={}, spotlight_dtype={}, embedding_name='embedding', label_name='label', prediction_name='prediction'):\n    sg = SliceGuard()\n    df_slices = sg.find_issues(\n        df,\n        categories,\n        label_name,\n        prediction_name,\n        accuracy_score,\n        precomputed_embeddings = {'embedding': df[embedding_name].to_numpy()},\n        metric_mode=\"max\",\n        feature_types=category_types\n    )\n\n    sg.report(spotlight_dtype=spotlight_dtype)\n\n    return df_slices\n```\n\n<\/details>\n\n## Step-by-step example on CIFAR-100\n\n### Load CIFAR-100 from Huggingface hub and convert it to Pandas dataframe\n\n```python\ndataset = datasets.load_dataset(\"renumics\/cifar100-enriched\", split=\"test\")\ndf = dataset.to_pandas()\n```\n\n### Enrich dataset with metadata using [Cleanvision](.\/cv_issues.mdx)\n\n```python\ndef cv_issues_cleanvision(df, image_name='image'):\n\n    image_paths = df['image'].to_list()\n    imagelab = Imagelab(filepaths=image_paths)\n    imagelab.find_issues()\n\n    df_cv=imagelab.issues.reset_index()\n\n    return df_cv\n\ndf_cv = cv_issues_cleanvision(df)\ndf = pd.concat([df, df_cv], axis=1)\n```\n\n### Identify and explore data slices with Sliceguard\n\n```python\ncategories=['dark_score', 'low_information_score', 'light_score', 'blurry_score', 'fine_label']\nprediction = 'fine_label_prediction'\nlabel = 'fine_label'\ncategory_types={'fine_label': 'nominal'}\nspotlight_dtype={\"image\": Image}\n\nfind_data_slices(df, categories, category_types=category_types, spotlight_dtype=spotlight_dtype, embedding_name='embedding', label_name=label, prediction_name=prediction)\n\n```","---\ntags: []\nid: text-classification\nslug: \/docs\/use-cases\/text-classification\n---\n\n# Detect Problems in Text Classification Datasets\n\nUse the [**sliceguard**](https:\/\/github.com\/Renumics\/sliceguard) library and [**Spotlight**](https:\/\/github.com\/Renumics\/spotlight) to quickly **detect problem clusters** that can cause issues when training your text classification model. Shows problems such as:\n\n1. Label Inconsistencies\n2. Outliers and Errors\n3. Text-specific Issues (e.g. typos, empty samples)\n\nGenerally it will show you everything that is hard to learn for a classification model. All you need is a dataframe containing **texts** and **labels**.\n\nFirst install the dependencies:\n\n```\npip install renumics-spotlight sliceguard[all] scikit-learn\n```\n\nThen run the following code to detect problematic clusters:\n\n```python\n# The Imports\nfrom renumics import spotlight\nfrom sliceguard import SliceGuard\nfrom sliceguard.data import from_huggingface\nfrom sklearn.metrics import accuracy_score\n\n# Load an Example Dataset as DataFrame\ndf = from_huggingface(\"dair-ai\/emotion\")\n\n# DataFrame Format:\n# +-------+-------+\n# | text  | label |\n# +-------+-------+\n# | text1 | joy   |\n# | text2 | anger |\n# | text3 | joy   |\n# | ...   |       |\n# +-------+-------+\n\n# Detect Issues Using sliceguard\nsg = SliceGuard()\nissues = sg.find_issues(df, features=[\"text\"], y=\"label\", metric=accuracy_score)\nreport_df, spotlight_data_issues, spotlight_dtypes, spotlight_layout = sg.report(\n    no_browser=True\n)\n\n# Visualize Detected Issues in Spotlight:\nspotlight.show(\n    report_df,\n    dtype=spotlight_dtypes,\n    issues=spotlight_data_issues,\n    layout=spotlight_layout,\n)\n```"],"8":["`audio(column, window_column=None, name=None, *, repeat=False, autoplay=False)` {audio()}\n\nAdd audio viewer to Spotlight inspector widget.\n\nSupports a single column of type `spotlight.Audio` with optional second\ncolumn of type `spotlight.Window`.","`spectrogram(column, window_column=None, name=None, *, frequency_scale='linear', amplitude_scale='decibel')` {spectrogram()}\n\nAdd audio spectrogram viewer to Spotlight inspector widget.\n\nSupports a single column of type `spotlight.Audio` with optional second\ncolumn of type `spotlight.Window`.","`viewers()` {viewers()}\n\nGet all active Spotlight viewer instances."],"9":["---\ntags: []\nid: inspector\nsidebar_position: 20\nslug: \/docs\/custom-visualizations\/ui-components\/inspector\n---\n\nimport Video from \"@site\/src\/components\/DocsVideo\";\n\n# Inspector\n\nThe Inspector Widget is a component in Spotlight that lets you examine and edit the features of **individual data points** in your dataset.\nWith its multiple views for **different modalities**, the Inspector Widget provides a detailed look at a data point's underlying structure.\n\nThe Inspector Widget's views for text, audio, video, and 3D geometry data allow you to explore a data point's features in depth.\nYou can also edit those features directly within the view, making it easy to make changes to a data point without navigating to a separate screen.\nThe Inspector Widget is an essential tool for working with multimodal datasets and gives you greater control over the structure and quality of your data.\n\n:::info\n\nThe open source version of Renumics Spotlight allows you to inspect your data to find critical segments. The Pro version of Spotlight also enables data tagging and annotation.\n\n:::\n\nBy configuring the inspector view you can choose how each datapoint should be represented. The main configuration options are available in the upper right corner of the view:\n\n<Video\n    basePath=\"\/data\/docs\/ui-components\/inspector-view\/inspector-view-overview\"\n    extensions={[\"webm\", \"mp4\", \"ogg\"]}\n    widths={[256, 512, 720, 1280]}\n    annotation=\"_fsd50k_ - add and layout components in the **Inspector View**\"\n\/>\n\nThere are visualization and interaction components available for many different data types. These include:\n\n### Text data\n\n**Text data** can be represented either with a **Value** view or a **Select** view that features autocomplete.\n\n<!-- poster=\"\/data\/docs\/ui-components\/inspector-view\/text-value-field\/text-value-field.jpg\" -->\n\n<Video\n    basePath=\"\/data\/docs\/ui-components\/inspector-view\/text-value-field\"\n    extensions={[\"webm\", \"mp4\", \"ogg\"]}\n    widths={[256, 512]}\n    isPro={false}\n    poster=\"\/data\/docs\/ui-components\/inspector-view\/text-value-field-512.jpg\"\n    annotation=\"_fsd50k_ - display annotation in a **Value** view\"\n\/>\n\n<Video\n    className=\"pt-10\"\n    basePath=\"\/data\/docs\/ui-components\/inspector-view\/text-select-field\"\n    extensions={[\"webm\", \"mp4\", \"ogg\"]}\n    widths={[256, 512]}\n    isPro={true}\n    poster=\"\/data\/docs\/ui-components\/inspector-view\/text-select-field.jpg\"\n    annotation=\"_fsd50k_ - display and update annotation in a **Select** view\"\n\/>\n\n### Categorical data\n\n**Categorical data** is represented similarly to **Text data** with the exception, that when there are not too many categories, they can also be edited with a switch component.\n\n<Video\n    className=\"pt-10\"\n    basePath=\"\/data\/docs\/ui-components\/inspector-view\/category-switch-field\"\n    extensions={[\"webm\", \"mp4\", \"ogg\"]}\n    widths={[256, 512]}\n    isPro={true}\n    poster=\"\/data\/docs\/ui-components\/inspector-view\/category-switch-field-512.jpg\"\n    annotation=\"_mnist_ - display and update categories in a **Switch** view\"\n\/>\n\n### Image data\n\n**Image data** can be represented with an imager viewer that supports zooming and panning.\n\n<Video\n    className=\"pt-10\"\n    basePath=\"\/data\/docs\/ui-components\/inspector-view\/image-view\"\n    extensions={[\"webm\", \"mp4\", \"ogg\"]}\n    widths={[256, 512]}\n    isPro={false}\n    poster=\"\/data\/docs\/ui-components\/inspector-view\/image-view-512.jpg\"\n    annotation=\"_mnist_ - display and zoom\/pan images in a **Image** view\"\n\/>\n\n### Audio data\n\n**Audio data** can be represented with an _Audio Player_ or a _Spectrogram_. The _Audio Player_ supports annotating event windows in the data.","---\nsidebar_position: 30\nslug: \/docs\/configure-visualizations\n---\n\n# Configuring visualizations\n\nData inspection is critical for all steps in the ML development workflow. But it is also time consuming: First of all it takes effort to build and maintain custom visualization pipelines. Second of all, it takes time to sift through data manually.\n\nSpotlight enables you to build scalable data inspection workflows. With Spotlight you can\n\n-   Create visualizations for unstructured data with a single line of code\n-   Leverage model results to find relevant data segments quickly\n-   Create custom interaction templates for your team\n\nIn order to achieve this, the Spotlight UI provides a range of components that offer different perspectives on your data. You can add, remove, and rearrange these components as you wish to create a customized inspection layouts.\n\nOnce you have identified a component layout that works well for you, you can save it as a layout for future use. This way, you can quickly load your preferred layout whenever you need it, without having to reconfigure the UI each time.\n\n## UI components\n\nSpotlight includes the [Inspector](ui_components\/inspector_view.mdx) and the [Data Table](ui_components\/table_view.mdx) to help you in analyzing and understanding individual datapoints. Other visualization options like the [Similarity Map](ui_components\/similarity_map.mdx), the [Scatter Plot](ui_components\/scatter_plot.mdx) and the [Histogram](ui_components\/histogram.mdx) aide you to understand the overall distribution of your data.\n\nThe [Filter Bar](ui_components\/filter_bar.mdx) is a special component that allows you to filter your data based on the values of your features.<br\/>\nFiltering and selecting datapoints can affect how data is presented in the other components and therefore can greatly help you in analyzing and reasoning about your data.\n\nAn overview over all available components can be found in the [UI components](ui_components\/index.mdx) section.\n\n## Layout API\n\n## Issues widget","---\ntitle: \ud83e\udd17 Loading Huggingface Data\nslug: \/docs\/loading-data\/huggingface\nsidebar_position: 30\n---\n\n# Loading Data from Huggingface datasets\n\n![Huggingface example](\/img\/speech_commands_vis_s.gif)\n\nThe Hugging Face datasets library not only provides access to more than 70k publicly available datasets, but also offers very convenient data preparation pipelines for custom datasets.\n\nRenumics Spotlight allows you to create interactive visualizations to identify critical clusters in your data. Because Spotlight understands the data semantics within Hugging Face datasets, you can get started with just one line of code:\n\n```python\nimport datasets\nfrom renumics import spotlight\n\nds = datasets.load_dataset('speech_commands', 'v0.01', split='validation')\n\nspotlight.show(ds)\n```\n\n## Spotlight \ud83e\udd1d Hugging Face datasets\n\nThe datasets library has several features that makes it an ideal tool for working with ML datasets: It stores tabular data (e.g. metadata, labels) along with unstructured data (e.g. images, audio) in a common Arrows table. Datasets also describes important data semantics through features (e.g. images, audio) and additional task-specific metadata.\n\nSpotlight directly works on top of the datasets library. This means that there is no need to copy or pre-process the dataset for data visualization and inspection. Spotlight loads the tabular data into memory to allow for efficient, client-side data analytics. Memory-intensive unstructured data samples (e.g. audio, images, video) are loaded lazily on demand. In most cases, data types and label mappings are inferred directly from the dataset. Here, we visualize the CIFAR-100 dataset with one line of code:\n\n```python\nds = datasets.load_dataset('cifar100', split='test')\nspotlight.show(ds)\n```\n\nIn cases where the data types are ambiguous or not specified, the Spotlight API allows to manually assign them:\n\n```python\nlabel_mapping = dict(zip(ds.features['fine_label'].names, range(len(ds.features['fine_label'].names))))\nspotlight.show(ds, dtype={'img': spotlight.Image, 'fine_label': spotlight.dtypes.CategoryDType(categories=label_mapping)})\n```\n\n## Supported data types\n\nMore details on the supported data types can be found in the [data types section](..\/loading-data#supported-data-types)\n\n## Detailed examples\n\nTake a look at our [use case section](..\/use-cases) to find more detailed examples for different modalities."],"10":["---\ntitle: \ud83e\udd17 Loading Huggingface Data\nslug: \/docs\/loading-data\/huggingface\nsidebar_position: 30\n---\n\n# Loading Data from Huggingface datasets\n\n![Huggingface example](\/img\/speech_commands_vis_s.gif)\n\nThe Hugging Face datasets library not only provides access to more than 70k publicly available datasets, but also offers very convenient data preparation pipelines for custom datasets.\n\nRenumics Spotlight allows you to create interactive visualizations to identify critical clusters in your data. Because Spotlight understands the data semantics within Hugging Face datasets, you can get started with just one line of code:\n\n```python\nimport datasets\nfrom renumics import spotlight\n\nds = datasets.load_dataset('speech_commands', 'v0.01', split='validation')\n\nspotlight.show(ds)\n```\n\n## Spotlight \ud83e\udd1d Hugging Face datasets\n\nThe datasets library has several features that makes it an ideal tool for working with ML datasets: It stores tabular data (e.g. metadata, labels) along with unstructured data (e.g. images, audio) in a common Arrows table. Datasets also describes important data semantics through features (e.g. images, audio) and additional task-specific metadata.\n\nSpotlight directly works on top of the datasets library. This means that there is no need to copy or pre-process the dataset for data visualization and inspection. Spotlight loads the tabular data into memory to allow for efficient, client-side data analytics. Memory-intensive unstructured data samples (e.g. audio, images, video) are loaded lazily on demand. In most cases, data types and label mappings are inferred directly from the dataset. Here, we visualize the CIFAR-100 dataset with one line of code:\n\n```python\nds = datasets.load_dataset('cifar100', split='test')\nspotlight.show(ds)\n```\n\nIn cases where the data types are ambiguous or not specified, the Spotlight API allows to manually assign them:\n\n```python\nlabel_mapping = dict(zip(ds.features['fine_label'].names, range(len(ds.features['fine_label'].names))))\nspotlight.show(ds, dtype={'img': spotlight.Image, 'fine_label': spotlight.dtypes.CategoryDType(categories=label_mapping)})\n```\n\n## Supported data types\n\nMore details on the supported data types can be found in the [data types section](..\/loading-data#supported-data-types)\n\n## Detailed examples\n\nTake a look at our [use case section](..\/use-cases) to find more detailed examples for different modalities.","---\nsidebar_position: 20\nslug: \/docs\/loading-data\n---\n\n# Loading data\n\nWith Spotlight you can interactively explore your unstructured data directly from your dataframe. When the data is loaded into Spotlight, the tabular data (e.g. labels, metadata) is loaded into memory and you can use the web frontend to perform efficient in-memory analytics. Unstructured data samples (e.g. images, video, audio, time series) are loaded lazily from disk or web storage.\n\n## Supported data formats\n\nSpotlight can be started either through the Python API or via the Command Line Interface (CLI). Three different dataset representations are supported: [Pandas](https:\/\/pandas.pydata.org\/) dataframes, [Huggingface datasets](https:\/\/huggingface.co\/docs\/datasets\/index) and Spotlight datasets based on the HDF5-Format.\n\nIf you load your dataset via CLI, you can specify a file to be loaded. In this example we load a CSV file:\n\n```bash\nspotlight mnist-tiny.csv\n```\n\nWith the Python API you can additionally load in-memory datasets. This is useful when working in a notebook. Loading a Pandas dataframe is as simple as:\n\n```python\nfrom renumics import spotlight\n\nspotlight.show(df)\n```\n\nThis table gives an overview over the supported data formats:\n\n| Format                                      | CLI | Python API |\n| ------------------------------------------- | --- | ---------- |\n| CSV, Parquet, Feather, ORC (through Pandas) | \u2705  | \u2705         |\n| Pandas in memory                            | \u274c  | \u2705         |\n| Huggingface                                 | \u274c  | \u2705         |\n| Spotlight HDF5                              | \u2705  | \u2705         |\n\n## Supported data types\n\nSpotlight supports a wide range of data types both for tabular and unstructured data types. When possible, the data types are automatically discovered.\n\nIt is also possible to manually specify data types for certain columns:\n\n```python\nfrom renumics import spotlight\n\ndtype = {\"image\": spotlight.Image, \"embedding\":spotlight.Embedding}\nspotlight.show(df, dtype=dtype)\n```\n\nWe provide a more detailed description vor both [tabular data types](loading-data\/data-types#tabular-data-types) and [unstructured data types](loading-data\/data-types#unstructured-data-types).\n\n## Load a Pandas dataset\n\nFind [more information how to load your Pandas dataframe](loading-data\/pandas) in just a few lines of code.\n\n## Huggingface\n\nFind [more information how to load your Hugging Face dataset](loading-data\/huggingface) in just a few lines of code.\n\n## Spotlight HDF5 dataset\n\nFind more information how to use the [Spotlight HDF5 dataset format](loading-data\/hdf5) to load complex multimdal data.","---\nslug: \/docs\/getting-started\nsidebar_position: 10\n---\n\nimport ReactPlayer from \"react-player\";\nimport Tabs from \"@theme\/Tabs\";\nimport TabItem from \"@theme\/TabItem\";\n\n# \ud83d\ude80 Getting Started\n\n> Spotlight helps you to **understand unstructured datasets** fast. You can create **interactive visualizations** from your dataframe with just a few lines of code. You can also leverage data enrichments (e.g. embeddings, prediction, uncertainties) to **identify critical clusters** in your data.\n\n![Dataframe example](\/img\/speech_commands_vis_s.gif)\n\n## What you'll need\n\n-   [Python](https:\/\/www.python.org\/downloads\/) version 3.8-3.11\n\n## Install Spotlight via [pip](https:\/\/packaging.python.org\/en\/latest\/key_projects\/#pip)\n\n```bash\npip install renumics-spotlight\n```\n\n> We recommend installing Spotlight and everything you need to work on your data in a separate [virtual environment](https:\/\/docs.python.org\/3\/tutorial\/venv.html)\n\n## Load your first dataset\n\nData can be loaded into Spotlight either from memory (as Python object) or from file. We currently support Pandas dataframes as well as Huggingface datasets and our own HDF5-based dataset format. Depending on the format, unstructured data samples are either stored directly in the dataframe or as a path to a separate file.\n\nThis is an example how your dataframe might look like:\n\n![Dataframe example](\/img\/dataframe_head_sample.png)\n\nYou can directly load your dataframe either via the Python API or the command line interface (CLI):\n\n<Tabs groupId=\"interface\">\n<TabItem value=\"python\" label=\"python\" default>\n\n```python\nimport pandas as pd\nfrom renumics import spotlight\n\ndf = pd.read_csv(\"https:\/\/renumics.com\/data\/mnist\/mnist-tiny.csv\")\nspotlight.show(df, dtype={\"image\": spotlight.Image, \"embedding\": spotlight.Embedding})\n```\n\n-   `pd.read_csv` loads a sample csv file as a pandas [DataFrame](https:\/\/pandas.pydata.org\/docs\/reference\/api\/pandas.DataFrame.html).\n-   `spotlight.show` opens up spotlight in the browser with the pandas dataframe ready for you to explore.\n-   The `dtype` argument specifies custom column types for the browser viewer.\n\n<\/TabItem>\n<TabItem value=\"bash\" label=\"CLI\">\n\n```bash\ncurl https:\/\/renumics.com\/data\/mnist\/mnist-tiny.csv -o mnist-tiny.csv\nspotlight mnist-tiny.csv --dtype image=Image --dtype embedding=Embedding\n```\n\n<\/TabItem>\n<\/Tabs>\n\n## Load a [Hugging Face](https:\/\/huggingface.co\/) dataset\n\nHuggingface datasets have a rich semantic description of the feature columns. Spotlight can thus parse data type descriptions and label mappings automatically. This means that creating a visualization is typically as simple as:\n\n```python\nimport datasets\nfrom renumics import spotlight\n\nds = datasets.load_dataset('speech_commands', 'v0.01', split='all')\n\nspotlight.show(ds)\n```\n\n## Exploring an enriched dataset with custom visualization layouts\n\nExploring raw unstructured datasets often yield little insights. Leveraging model results such as predictions or embeddings can help to uncover critical data samples and clusters. In practice, these enrichments can be stored in a separate dataframe and then joined with the raw dataset. Here is an example from the Huggingface hub:\n\n```python\nimport datasets\n\nds = datasets.load_dataset('speech_commands', 'v0.01', split='all')\nds_results = datasets.load_dataset('renumics\/speech_commands-ast-finetuned-results', 'v0.01', split='all')\nds = datasets.concatenate_datasets([ds, ds_results], axis=1)\n```\n\nDepending on the task at hand (e.g. EDA, model debugging, monitoring), you probably want to set up a suitable visualization layout. You can do so in the GUI and via API."]}}